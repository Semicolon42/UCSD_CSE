{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "# == \n",
    "# == \n",
    "# == \n",
    "\n",
    "import nltk;\n",
    "import timeit\n",
    "# nltk.download()\n",
    "\n",
    "def done():\n",
    "    print \"\\n===DONE===\"\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Brown Words:  <class 'nltk.corpus.reader.util.ConcatenatedCorpusView'> 1161192 \n",
      "\n",
      "====STOP WORDS====\n",
      "set([u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'])\n",
      "\n",
      "Regex Pattern Requirement: (.*)[a-zA-Z]+(.*)\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "# == \n",
    "# == \n",
    "# == \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# ==== The Words ====\n",
    "brown_words = nltk.corpus.brown.words();\n",
    "print \"Corpus Brown Words: \", type(brown_words), len(brown_words), \"\\n\"\n",
    "\n",
    "# ==== STOP WORDS ====\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print \"====STOP WORDS====\\n\",stop_words\n",
    "\n",
    "# ==== MUST CONTAIN AN ALPHABET CHARACTER ====\n",
    "regex = \"(.*)[a-zA-Z]+(.*)\"\n",
    "regex_pattern = re.compile(regex)\n",
    "print \"\\nRegex Pattern Requirement: \" + regex\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-Of-Words size 47892 , Top 10:  [[u'one', 3292], [u'would', 2714], [u'said', 1961], [u'new', 1635], [u'could', 1601], [u'time', 1598], [u'two', 1412], [u'may', 1402], [u'first', 1361], [u'like', 1292]]\n",
      "\n",
      "Unfilteres Words: 1161192 Filtered: 530090\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "# == \n",
    "# == \n",
    "# == \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "debug = False\n",
    "\n",
    "# Initialize Word Counts\n",
    "wc = dict()\n",
    "words = []\n",
    "\n",
    "#Building Word Counts\n",
    "j = 0\n",
    "s = \"\"\n",
    "for i in range(len(brown_words)):\n",
    "    w = brown_words[i].lower()\n",
    "    if w not in stop_words and regex_pattern.match(w):\n",
    "        words.append(w)\n",
    "        wc[w] = wc.get(w, 0) + 1\n",
    "        \n",
    "        if debug:\n",
    "            j+=1\n",
    "            s = s + \" \" + w\n",
    "            if j > 100:\n",
    "                print s\n",
    "                s = \"\"\n",
    "                j=0\n",
    "\n",
    "swc = []\n",
    "for key, value in sorted(wc.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "    swc.append([key, value])\n",
    "                \n",
    "print \"\\nBag-Of-Words size\", len(swc), \", Top 10: \", swc[:10]\n",
    "print \"\\nUnfilteres Words:\",len(brown_words),\"Filtered:\",len(words)\n",
    "\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Words  5000 , Top 10:  [u'one', u'would', u'said', u'new', u'could', u'time', u'two', u'may', u'first', u'like']\n",
      "Vocabulary index_map  5000 one <type 'unicode'>\n",
      "\n",
      "Context Words  1000 , Top 10:  [[u'one', 3292], [u'would', 2714], [u'said', 1961], [u'new', 1635], [u'could', 1601], [u'time', 1598], [u'two', 1412], [u'may', 1402], [u'first', 1361], [u'like', 1292]]\n",
      "Context Words  1000 , Top 10:  [u'one', u'would', u'said', u'new', u'could', u'time', u'two', u'may', u'first', u'like']\n",
      "Context index_map  1000 one <type 'unicode'>\n",
      "\n",
      "Word Window [None, None, None, u'fulton', u'county'] None [0, 1, 3, 4] [0, 1, 3, 4]\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "# == \n",
    "# == Getting Vocab sets\n",
    "# == \n",
    "\n",
    "vocabulary_size = 5000\n",
    "V = swc[:vocabulary_size]\n",
    "V_words = []; \n",
    "for v in V: \n",
    "    V_words.append(v[0])\n",
    "print \"Vocabulary Words \", len(V_words), \", Top 10: \", V_words[:10]\n",
    "V_index_map = dict()\n",
    "for i in range(len(V_words)):\n",
    "    V_index_map[V_words[i]] = i\n",
    "print \"Vocabulary index_map \", len(V_index_map), V_words[0], type(V_words[0])\n",
    "\n",
    "\n",
    "context_size = 1000\n",
    "C = swc[:context_size]\n",
    "C_words = []\n",
    "for c in C:\n",
    "    C_words.append(c[0])\n",
    "print \"\\nContext Words \", len(C), \", Top 10: \", C[:10]\n",
    "print \"Context Words \", len(C_words), \", Top 10: \", C_words[:10]\n",
    "\n",
    "C_index_map = dict()\n",
    "for i in range(len(C_words)):\n",
    "    C_index_map[C_words[i]] = i\n",
    "print \"Context index_map \", len(C_index_map), C_words[0], type(C_words[0])\n",
    "\n",
    "\n",
    "window = 2\n",
    "word_window_words = [None]*(len(words)+window*2)\n",
    "word_window_words[window:-window] = words\n",
    "word_window = [None]*(window+window+1)\n",
    "word_window[(window+1):] = words[:window]\n",
    "window_range = range(window+window+1)\n",
    "del window_range[window]\n",
    "print \"\\nWord Window\", word_window, word_window[window], window_range[:5], window_range[-5:]\n",
    "\n",
    "\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TEST> one <type 'unicode'> <_sre.SRE_Match object at 0x10f7cdcf0>\n",
      "<TEST> . <type 'unicode'> None\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "# Test bed\n",
    "temp = u'one'\n",
    "print \"<TEST>\", temp, type(temp), regex_pattern.match(temp)\n",
    "temp = u'.'\n",
    "print \"<TEST>\", temp, type(temp), regex_pattern.match(temp)\n",
    "\n",
    "\n",
    "\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: <type 'list'> <type 'list'> [[u'one', 3292], [u'would', 2714], [u'said', 1961], [u'new', 1635], [u'could', 1601]]\n",
      "Word Context: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf64a47953a4957a1739d6aab4c70bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1179459bf4eb4af7aa8eba8926ce7e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>FloatProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "FloatProgress(value=0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 1521040072.91\n",
      "Stop: 1521040663.35\n",
      "Time: 590.436729908\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "# == \n",
    "# == \n",
    "# == \n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "import timeit\n",
    "\n",
    "\n",
    "print \"C:\", type(C),type(C[0]),C[:5]\n",
    "word_context = {\"word\":[]}\n",
    "print \"Word Context:\",len(word_context)\n",
    "\n",
    "words_length = len(words)\n",
    "f = FloatProgress(min=0, max=100)\n",
    "display(f)\n",
    "ff = FloatProgress(min=0, max=100)\n",
    "display(ff)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "print \"Start:\",start\n",
    "# for index in range(2000):\n",
    "for index in range(words_length):\n",
    "    f.value = index / float(words_length)*100\n",
    "    ff.value = (index-words_length+1000.)/1000.*100.\n",
    "    word = words[index]\n",
    "    if word not in V_words:\n",
    "        continue;\n",
    "    word_window = word_window_words[index:index+window+window+1];\n",
    "    word_context[word] = [0]*len(C_words)\n",
    "    for i in window_range:\n",
    "        if word_window[i] in C_words:\n",
    "            cw = word_window[i]\n",
    "            word_context[word][C_index_map[cw]] += 1\n",
    "            if DEBUG and sum(word_context[word]) > 1:\n",
    "                print word,cw,word_context[word][C_index_map[cw]],sum(word_context[word])\n",
    "#             if word == cw:\n",
    "#                 print word_window\n",
    "stop = timeit.default_timer()\n",
    "print \"Stop:\",stop\n",
    "print \"Time:\",stop - start \n",
    "\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> 5000 [[u'zero', 4], [u'youngsters', 4], [u'works', 4], [u\"women's\", 4], [u'women', 4]]\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# print len(word_context), word_context.keys()[:5],[sum(li) for li in word_context.values()[:5]]\n",
    "# sorted_word_context = []\n",
    "# for key, value in sorted(word_context.iteritems(), key=lambda (k,v): (sum(v),k), reverse=True):\n",
    "#     sorted_word_context.append([key, sum(value),value])\n",
    "# \n",
    "# print len(sorted_word_context),[row[:2] for row in sorted_word_context[:5]]\n",
    "# \n",
    "# with open(\"sorted_word_context.txt\", 'wb') as dump:\n",
    "#     dump.write(json.dumps(sorted_word_context))\n",
    "with open(\"sorted_word_context.txt\", 'rb') as dump:\n",
    "    lines = json.loads(dump.read())\n",
    "sorted_word_context = lines\n",
    "\n",
    "# with open(\"sorted_word_context.txt\", 'w') as out_file:\n",
    "#     out_file.write(str(sorted_word_context))\n",
    "\n",
    "# with open('sorted_word_context.txt') as f:\n",
    "#     lines = f.read()\n",
    "print type(lines),len(lines),[row[:2] for row in lines[:5]]\n",
    "    \n",
    "\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234748.0\n",
      "[0.014023548656431578, 0.011561333855879496, 0.00835363879564469, 0.006964915569035732, 0.006820079404297374] <type 'list'>\n",
      "5000 1000\n",
      "X [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      "===DONE===\n"
     ]
    }
   ],
   "source": [
    "n = float(sum([wc[c] for c in C_words]))\n",
    "print n\n",
    "prob_c = [wc[c]/n for c in C_words]\n",
    "print prob_c[:5], type(prob_c)\n",
    "\n",
    "rows_height, columns_width = 20, 10\n",
    "rows_height, columns_width = len(V_words),len(C_words)\n",
    "print rows_height, columns_width \n",
    "X = np.zeros((rows_height,columns_width))\n",
    "PX = np.zeros((rows_height,columns_width))\n",
    "for x in range(rows_height):\n",
    "    X[x,:] = sorted_word_context[x][2][:columns_width]\n",
    "print \"X\", X\n",
    "# print \"\\n\\nWWWWWWWWWWWWWWWWWWWWWWW\\n\\n\"\n",
    "# print PX\n",
    "\n",
    "done() # ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  4.  0.]\n",
      " [ 1.  4.  0.]\n",
      " [ 2.  3.  5.]\n",
      " [ 2.  4.  5.]\n",
      " [ 3.  2.  0.]\n",
      " [ 4.  1.  0.]]\n",
      "\n",
      "\n",
      "Sum of WC rows [4.0, 5.0, 10.0, 11.0, 5.0, 5.0] \n",
      "Total Sum 40.0\n",
      "Context word probabilities [0.10000000000000001, 0.125, 0.25, 0.27500000000000002, 0.125, 0.125] \n",
      "log of prob_c [-2.30258509 -2.07944154 -1.38629436 -1.29098418 -2.07944154 -2.07944154] [[ 0.1    0.1    0.1  ]\n",
      " [ 0.125  0.125  0.125]\n",
      " [ 0.25   0.25   0.25 ]\n",
      " [ 0.275  0.275  0.275]\n",
      " [ 0.125  0.125  0.125]\n",
      " [ 0.125  0.125  0.125]] \n",
      "[[-2.30258509 -2.30258509 -2.30258509]\n",
      " [-2.07944154 -2.07944154 -2.07944154]\n",
      " [-1.38629436 -1.38629436 -1.38629436]\n",
      " [-1.29098418 -1.29098418 -1.29098418]\n",
      " [-2.07944154 -2.07944154 -2.07944154]\n",
      " [-2.07944154 -2.07944154 -2.07944154]] \n",
      "\n",
      "\n",
      "A [[ 0.          1.          0.        ]\n",
      " [ 0.2         0.8         0.        ]\n",
      " [ 0.2         0.3         0.5       ]\n",
      " [ 0.18181818  0.36363636  0.45454545]\n",
      " [ 0.6         0.4         0.        ]\n",
      " [ 0.8         0.2         0.        ]]\n",
      "A1 [[       -inf  0.                -inf]\n",
      " [-1.60943791 -0.22314355        -inf]\n",
      " [-1.60943791 -1.2039728  -0.69314718]\n",
      " [-1.70474809 -1.01160091 -0.78845736]\n",
      " [-0.51082562 -0.91629073        -inf]\n",
      " [-0.22314355 -1.60943791        -inf]]\n",
      "A2 [[ 0.          0.          0.        ]\n",
      " [-1.60943791 -0.22314355  0.        ]\n",
      " [-1.60943791 -1.2039728  -0.69314718]\n",
      " [-1.70474809 -1.01160091 -0.78845736]\n",
      " [-0.51082562 -0.91629073  0.        ]\n",
      " [-0.22314355 -1.60943791  0.        ]]\n",
      "B [[-2.30258509 -2.30258509 -2.30258509]\n",
      " [-2.07944154 -2.07944154 -2.07944154]\n",
      " [-1.38629436 -1.38629436 -1.38629436]\n",
      " [-1.29098418 -1.29098418 -1.29098418]\n",
      " [-2.07944154 -2.07944154 -2.07944154]\n",
      " [-2.07944154 -2.07944154 -2.07944154]]\n",
      "C [[ 0.          0.          0.        ]\n",
      " [ 0.77397603  0.10730936  0.        ]\n",
      " [ 1.16096405  0.8684828   0.5       ]\n",
      " [ 1.3205027   0.78358893  0.6107413 ]\n",
      " [ 0.2456552   0.4406427   0.        ]\n",
      " [ 0.10730936  0.77397603  0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bmehaffy/anaconda/envs/tensorflow2.7/lib/python2.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wc_matrix = np.array([[0.,1.,2.,2.,3.,4.],[4.,4.,3.,4.,2.,1.],[0.,0.,5.,5.,0.,0.]])\n",
    "wc_matrix = np.transpose(wc_matrix)\n",
    "print wc_matrix\n",
    "sum_wc_rows = [sum(wc_matrix[i,:]) for i in range(wc_matrix.shape[0])]\n",
    "sum_wc = sum(sum_wc_rows)\n",
    "prob_c = [float(row_sum)/sum_wc for row_sum in sum_wc_rows]\n",
    "prob_c_matrix = np.transpose(np.tile(prob_c,(wc_matrix.shape[1],1)))\n",
    "log_prob_c_matrix = np.log(prob_c_matrix)\n",
    "print \"\\n\\nSum of WC rows\",sum_wc_rows,\"\\nTotal Sum\", sum_wc\n",
    "print \"Context word probabilities\", prob_c,\"\\nlog of prob_c\",np.log(prob_c),prob_c_matrix,\"\\n\",log_prob_c_matrix,\"\\n\\n\"\n",
    "\n",
    "wc_prob_matrix = np.zeros(wc_matrix.shape)\n",
    "for i in range(wc_matrix.shape[0]):\n",
    "    wc_prob_matrix[i,:] = (wc_matrix[i,:]/sum_wc_rows[i])\n",
    "print \"A\", wc_prob_matrix\n",
    "log_wc_prob_matrix = np.log(wc_prob_matrix)\n",
    "print \"A1\",log_wc_prob_matrix\n",
    "log_wc_prob_matrix[log_wc_prob_matrix==-np.inf]=0\n",
    "print \"A2\",log_wc_prob_matrix\n",
    "print \"B\", log_prob_c_matrix\n",
    "phi_matrix = np.maximum(np.zeros(log_wc_prob_matrix.shape),log_wc_prob_matrix/log_prob_c_matrix)\n",
    "print \"C\", phi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(phi_matrix)\n",
    "print pca.components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
